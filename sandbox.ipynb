{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9eb8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import datetime\n",
    "\n",
    "client = arxiv.Client()\n",
    "\n",
    "today = datetime.date.today()\n",
    "one_week_ago = today - datetime.timedelta(days=7)\n",
    "week_range = f\"{one_week_ago.strftime('%Y%m%d')}0000 TO {today.strftime('%Y%m%d')}2359\"\n",
    "query = f\"cat:cs.CL AND submittedDate:[{week_range}]\"\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = query,\n",
    "  max_results = 100,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e114545a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat:cs.CL AND submittedDate:[202510060000 TO 202510132359]'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94da618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamingVLM: Real-Time Understanding for Infinite Video Streams\n",
      "Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation\n",
      "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?\n",
      "Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in Spoken Language Models\n",
      "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents\n",
      "AutoPR: Let's Automate Your Academic Promotion!\n",
      "WUGNECTIVES: Novel Entity Inferences of Language Models from Discourse Connectives\n",
      "A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages\n",
      "Hierarchical Indexing with Knowledge Enrichment for Multilingual Video Corpus Retrieval\n",
      "Beyond Surface Reasoning: Unveiling the True Long Chain-of-Thought Capacity of Diffusion Large Language Models\n",
      "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models\n",
      "Evaluating Robustness of Large Language Models Against Multilingual Typographical Errors\n",
      "Mitigating Overthinking through Reasoning Shaping\n",
      "Accent-Invariant Automatic Speech Recognition via Saliency-Driven Spectrogram Masking\n",
      "Can We Reliably Rank Model Performance across Domains without Labeled Data?\n",
      "StatEval: A Comprehensive Benchmark for Large Language Models in Statistics\n",
      "Multimodal Policy Internalization for Conversational Agents\n",
      "Hybrid Models for Natural Language Reasoning: The Case of Syllogistic Logic\n",
      "Getting Your Indices in a Row: Full-Text Search for LLM Training Data for Real World\n",
      "Domain-Adapted Pre-trained Language Models for Implicit Information Extraction in Crash Narratives\n",
      "KORMo: Korean Open Reasoning Model for Everyone\n",
      "The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach\n",
      "On the Representations of Entities in Auto-regressive Large Language Models\n",
      "Active Model Selection for Large Language Models\n",
      "Estimating Brain Activity with High Spatial and Temporal Resolution using a Naturalistic MEG-fMRI Encoding Model\n",
      "Beyond Single-Granularity Prompts: A Multi-Scale Chain-of-Thought Prompt Learning for Graph\n",
      "Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation\n",
      "HINT: Helping Ineffective Rollouts Navigate Towards Effectiveness\n",
      "Token-Level Policy Optimization: Linking Group-Level Rewards to Token-Level Aggregation via Markov Likelihood\n",
      "Understanding the Effects of Domain Finetuning on LLMs\n",
      "NL2GenSym: Natural Language to Generative Symbolic Rules for SOAR Cognitive Architecture via Large Language Models\n",
      "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training\n",
      "ReTraceQA: Evaluating Reasoning Traces of Small Language Models in Commonsense Question Answering\n",
      "LLP: LLM-based Product Pricing in E-commerce\n",
      "FLRC: Fine-grained Low-Rank Compressor for Efficient LLM Inference\n",
      "Large Language Model Prompt Datasets: An In-depth Analysis and Insights\n",
      "Verifying Chain-of-Thought Reasoning via Its Computational Graph\n",
      "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference\n",
      "Target speaker anonymization in multi-speaker recordings\n",
      "CapGeo: A Caption-Assisted Approach to Geometric Reasoning\n",
      "ShiZhi: A Chinese Lightweight Large Language Model for Court View Generation\n",
      "MaP: A Unified Framework for Reliable Evaluation of Pre-training Dynamics\n",
      "One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations\n",
      "CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts\n",
      "Inflated Excellence or True Performance? Rethinking Medical Diagnostic Benchmarks with Dynamic Evaluation\n",
      "CFVBench: A Comprehensive Video Benchmark for Fine-grained Multimodal Retrieval-Augmented Generation\n",
      "Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models\n",
      "DSPO: Stable and Efficient Policy Optimization for Agentic Search and Reasoning\n",
      "CrisiText: A dataset of warning messages for LLM training in emergency communication\n",
      "Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras\n",
      "Unsupervised lexicon learning from speech is limited by representations rather than clustering\n",
      "IRIS: An Iterative and Integrated Framework for Verifiable Causal Discovery in the Absence of Tabular Data\n",
      "DICE: Structured Reasoning in LLMs through SLM-Guided Chain-of-Thought Correction\n",
      "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs\n",
      "LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning\n",
      "Stronger Re-identification Attacks through Reasoning and Aggregation\n",
      "Augmenting Dialog with Think-Aloud Utterances for Modeling Individual Personality Traits by LLM\n",
      "DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation\n",
      "When Retrieval Succeeds and Fails: Rethinking Retrieval-Augmented Generation for LLMs\n",
      "FrameEOL: Semantic Frame Induction using Causal Language Models\n",
      "Exploiting Web Search Tools of AI Agents for Data Exfiltration\n",
      "ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability\n",
      "Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation\n",
      "Auto-scaling Continuous Memory for GUI Agent\n",
      "Large Language Models Do NOT Really Know What They Don't Know\n",
      "Exploring Cross-Lingual Knowledge Transfer via Transliteration-Based MLM Fine-Tuning for Critically Low-resource Chakma Language\n",
      "Automated Refinement of Essay Scoring Rubrics for Language Models via Reflect-and-Revise\n",
      "LitE-SQL: A Lightweight and Efficient Text-to-SQL Framework with Vector-based Schema Linking and Execution-Guided Self-Correction\n",
      "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models\n",
      "Decoupling Safety into Orthogonal Subspace: Cost-Efficient and Performance-Preserving Alignment for Large Language Models\n",
      "DARO: Difficulty-Aware Reweighting Policy Optimization\n",
      "MASA: LLM-Driven Multi-Agent Systems for Autoformalization\n",
      "Creation of the Chinese Adaptive Policy Communication Corpus\n",
      "Diagnosing and Mitigating System Bias in Self-Rewarding RL\n",
      "Semantic-Condition Tuning: Fusing Graph Context with Large Language Models for Knowledge Graph Completion\n",
      "Unleashing Perception-Time Scaling to Multimodal Reasoning Models\n",
      "A Human Behavioral Baseline for Collective Governance in Software Projects\n",
      "SOP-Maze: Evaluating Large Language Models on Complicated Business Standard Operating Procedures\n",
      "Artificial Impressions: Evaluating Large Language Model Behavior Through the Lens of Trait Impressions\n",
      "Autoencoding-Free Context Compression for LLMs via Contextual Semantic Anchors\n",
      "A Unified Biomedical Named Entity Recognition Framework with Large Language Models\n",
      "HES-SQL: Hybrid Reasoning for Efficient Text-to-SQL with Structural Skeleton Guidance\n",
      "Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR\n",
      "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs\n",
      "ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling\n",
      "Quality Estimation Reranking for Document-Level Translation\n",
      "ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review\n",
      "Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models\n",
      "Time-Aware Feature Selection: Adaptive Temporal Masking for Stable Sparse Autoencoder Training\n",
      "Everyone prefers human writers, including AI\n",
      "McMining: Automated Discovery of Misconceptions in Student Code\n",
      "Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs\n",
      "The Model's Language Matters: A Comparative Privacy Analysis of LLMs\n",
      "MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding\n",
      "Benchmarking Chinese Commonsense Reasoning with a Multi-hop Reasoning Perspective\n",
      "Learning What to Remember: Adaptive Probabilistic Memory Retention for Memory-Efficient Language Models\n",
      "Measuring Moral LLM Responses in Multilingual Capacities\n",
      "Struc-EMB: The Potential of Structure-Aware Encoding in Language Embeddings\n",
      "A Design-based Solution for Causal Inference with Text: Can a Language Model Be Too Large?\n",
      "Robust Heuristic Algorithm Design with LLMs\n"
     ]
    }
   ],
   "source": [
    "for r in client.results(search):\n",
    "  print(r.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2eb162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
